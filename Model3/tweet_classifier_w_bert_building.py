# -*- coding: utf-8 -*-
"""tweet_classifier_w/BERT_building.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w2JhA373A4GbLds_k-pIJZzqfpBYsPbB
"""

# ! pip install text_hammer
#
# !pip install -q tensorflow_text==2.9.0
#
# !pip install -q tensorflow==2.9.0
#
# !pip install -q tensorflow_estimator==2.9.0

# from google.colab import files

# files.upload()

import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
import text_hammer as th
import numpy as np
import pandas as pd


import os
import csv
import re

from sklearn.model_selection import train_test_split

pwd = os.getcwd()

TEST_PATH = pwd + "/new_train.csv"

TEST_SIZE = 0.2


def parse_data(filepath):

    with open(filepath, encoding="utf8") as file:

        tweets = []
        labels = []

        csv_reader = csv.reader(file)

        next(csv_reader)

        for row in csv_reader:

            tweet = row[0]
            label = row[1]

            tweets.append(tweet)
            labels.append(label)

    return tweets, labels


TWEETS, LABELS = parse_data(TEST_PATH)


def clean_data(ori_tweets):

    tweets = []

    for tweet in ori_tweets:
        tweet = str(tweet).lower().replace('\\', '').replace('_', ' ')
        tweet = th.cont_exp(tweet)
        tweet = th.remove_emails(tweet)
        tweet = re.sub(r'[^\w]', ' ', tweet)
        tweet = th.remove_rt(tweet)
        tweet = th.remove_accented_chars(tweet)
        tweet = th.remove_special_chars(tweet)
        tweet = re.sub("(.)\\1{2,}", "\\1", tweet)

        tweets.append(tweet)

    tweets = np.array(tweets).reshape(-1, 1)

    return tweets


TWEETS = clean_data(TWEETS)
LABELS = np.array(LABELS).astype("float32")


def split_data(tweets, labels, test_size):

    train_tweets, val_tweets, train_labels, val_labels = train_test_split(tweets, labels, test_size=test_size,
                                                                          random_state=42)

    return train_tweets, val_tweets, train_labels, val_labels


TRAIN_TWEETS, VAL_TWEETS, TRAIN_LABELS, VAL_LABELS = split_data(TWEETS, LABELS, TEST_SIZE)


@tf.function
def f1_score(y, yhat, thresh=0.5):

    y_pred = tf.cast(tf.greater(yhat, thresh), tf.float32)
    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)
    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)
    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)
    f1 = 2 * tp / (2 * tp + fn + fp + 1e-16)
    score = tf.reduce_mean(f1)

    return score


BERT_PREPROCESSOR = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
BERT_ENCODER = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4", trainable=False)


def create_model(preprocessor, encoder):
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
    encoder_inputs = preprocessor(text_input)
    outputs = encoder(encoder_inputs)
    pooled_output = outputs["pooled_output"]
    dropout = tf.keras.layers.Dropout(0.1, name="dropout")(pooled_output)
    dense = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)

    model = tf.keras.Model(inputs=[text_input], outputs=[dense])

    model.compile(loss="binary_crossentropy",
                  optimizer="adam",
                  metrics=[f1_score])

    return model


MODEL = create_model(BERT_PREPROCESSOR, BERT_ENCODER)

history = MODEL.fit(TRAIN_TWEETS, TRAIN_LABELS, epochs=3, validation_data=(VAL_TWEETS, VAL_LABELS))


f1_score = history.history["f1_score"]
val_f1_score = history.history["val_f1_score"]

loss = history.history["loss"]
val_loss = history.history["val_loss"]

epochs = range(len(f1_score))

plt.plot(epochs, f1_score, "red", label="Training F1 Score")
plt.plot(epochs, val_f1_score, "blue", label="Validation F1 Score")
plt.title("Training and Validation Accuracies")

plt.plot(epochs, loss, "red", ls="--", label="Training Loss")
plt.plot(epochs, val_loss, "blue", ls="--", label="Validation Loss")
plt.title("Training and Validation Losses")

plt.legend()

plt.show()

MODEL.save("tweet_classifier.h5")

# files.download("tweet_classifier.h5")

"""# Make Predictions"""


# files.upload()

TEST_PATH = pwd + "/test.csv"
SAMPLE_PATH = pwd + "/sample_submission.csv"


def parse_test_data(filepath):

    with open(filepath, encoding="utf8") as file:

        tweets = []

        csv_reader = csv.reader(file)

        next(csv_reader)

        for row in csv_reader:

            tweet = row[-1]

            tweets.append(tweet)

    return tweets


TEST_TWEETS = parse_test_data(TEST_PATH)
TEST_TWEETS = clean_data(TEST_TWEETS)

sample_submission = pd.read_csv(SAMPLE_PATH)

predictions = MODEL.predict(TEST_TWEETS)

predictions = [0 if x < 0.5 else 1 for x in predictions]

sample_submission["target"] = predictions

sample_submission.to_csv("submission.csv", index=False)

# files.download("submission.csv")
